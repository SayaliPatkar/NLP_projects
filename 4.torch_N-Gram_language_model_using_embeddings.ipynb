{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x25c4cc35a90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tri-grams context len gth is 2, 3rd word must relate to first two words\n",
    "CONTEXT_SIZE = 2\n",
    "\n",
    "# 10 dimensional embeddings, embedding vector has size [n,10], n = number of words in vocabulary\n",
    "EMBEDDING_DIM = 10\n",
    "\n",
    "# Para from American Gods\n",
    "test_sentence = \"\"\"That is the tale; the rest is detail.There are stories that are true, in which each individual’s \n",
    "tale is unique and tragic, and the worst of the tragedy is that we have heard it before, and we cannot allow ourselves \n",
    "to feel it too deeply. We build a shell around it like an oyster dealing with a painful particle of grit, coating it with \n",
    "smooth pearl layers in order to cope. This is how we walk and talk and function, day in, day out, immune to others’ \n",
    "pain and loss. If it were to touch us it would cripple us or make saints of us; but for the most part, it does not touch us. \n",
    "We cannot allow it to. Tonight, as you eat, reflect if you can: there are children starving in the world, starving in \n",
    "numbers larger than the mind can easily hold, up in the big numbers where an error of a million here, a million there, \n",
    "can be forgiven. It may be uncomfortable for you to reflect upon this or it may not, but still, you will eat. There are \n",
    "accounts which, if we open our hearts to them, will cut us too deeply. Look — here is a good man, good by his own \n",
    "lights and the lights of his friends: he is faithful and true to his wife, he adores and lavishes attention on his little \n",
    "children, he cares about his country, he does his job punctiliously, as best he can. So, efficiently and good-naturedly, \n",
    "he exterminates Jews: he appreciates the music that plays in the background to pacify them; he advises the Jews not to forget \n",
    "their identification numbers as they go into the showers — many people, he tells them, forget their numbers, and take the \n",
    "wrong clothes, when they come out of the showers. This calms the Jews: there will be life, they assure themselves, after the \n",
    "showers. And they are wrong. Our man supervises the detail taking the bodies to the ovens; and if there is anything he \n",
    "feels bad about, it is that he still allows the gassing of vermin to affect him. Were he a truly good man, he knows, he \n",
    "would feel nothing but joy, as the earth is cleansed of its pests.\n",
    "Leave him; he cuts too deep. He is too close to us and it hurts.\n",
    "Women and men, the old and the young of them: there are so many of them, and so many of their stories are tragedies with \n",
    "griefs too deep to be contained, but holding here and there tiny joys, snatched from the darkness like flowers picked by the \n",
    "fallen traveler from the side of a cliff.\n",
    "No man, proclaimed Donne, is an Island, and he was wrong. If we were not islands, we would be lost, drowned in each other’s \n",
    "tragedies. We are insulated (a word that means, literally, remember, made into an island) from the tragedy of others, by our \n",
    "island nature, and by the repetitive shape and form of the stories. We know the shape, and the shape does not change. \n",
    "There was a human being who was born, lived, and then, by some means or other, died. there. You may fill in the details from \n",
    "your own experience. As unoriginal as any other tale, as unique as any other life. Lives are snowflakes — unique in detail, \n",
    "forming patterns we have seen before, but as like one another as peas in a pod (and have you ever looked at peas in a pod? \n",
    "I mean, really looked at them? There’s not a chance you’d mistake one for another, after a minute’s close inspection.)\n",
    "We need individual stories. Without individuals we see only numbers: a thousand dead, a hundred thousand dead, ‘casualties \n",
    "may rise to a million’. With individual stories, the statistics become people — but even that is a lie, for the people \n",
    "continue to suffer in numbers that themselves are numbing and meaningless. Look, see the child’s swollen, swollen belly, \n",
    "and the flies that crawl at the corners of his eyes, his skeletal limbs: will it make it easier for you to know his name, \n",
    "his age, his dreams, his fears? To see him from the inside? And if it does, are we not doing a disservice to his sister, \n",
    "who lies in the searing dust beside him, a distorted, distended caricature of a human child. And there, if we feel for them, \n",
    "are they now more important to us than a thousand other children touched by the same famine, a thousand other young lives \n",
    "who will soon be food for the flies’ own myriad squirming children?\n",
    "We draw our lines around these moments of pain, and remain upon our islands, and they cannot hurt us. They are covered with \n",
    "a smooth, safe, nacreous layer to let them slip, pearl-like, from our souls without real pain. Fiction allows us to slide into \n",
    "these other heads, these other places, and look out through other eyes. And then in the tale we stop before we die, or we \n",
    "die vicariously and unharmed, and in the world beyond the tale we turn the page or close the book, and we resume our lives.\n",
    "A life, which is, like any other, unlike any other.\"\"\".split()\n",
    "\n",
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length :::  438\n",
      "Generated trigrams :::  [(['That', 'is'], 'the'), (['is', 'the'], 'tale;'), (['the', 'tale;'], 'the')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary length ::: \", len(vocab))\n",
    "\n",
    "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
    "            for i in range(len(test_sentence) - 2)]\n",
    "\n",
    "# print the first 3, just so you can see what they look like\n",
    "print(\"Generated trigrams ::: \", trigrams[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5291.958989620209, 5225.31563949585, 5160.828944206238, 5098.625154733658, 5039.238744497299, 4983.467415809631, 4931.942175149918, 4884.7227511405945, 4841.274475693703, 4800.869780898094, 4762.980112910271, 4727.252022862434, 4693.421999454498, 4661.301763653755, 4630.659889936447, 4601.3033452034, 4573.040971755981, 4545.709414839745, 4519.113343119621, 4493.106626152992, 4467.554193854332, 4442.33916926384, 4417.348442077637, 4392.509579539299, 4367.759881019592, 4343.048964858055, 4318.318013429642, 4293.5517427921295, 4268.724505782127, 4243.820360898972, 4218.802584767342, 4193.656110405922, 4168.3736045360565, 4142.905546545982, 4117.238303303719, 4091.349445581436, 4065.2250306606293, 4038.8631279468536, 4012.2589473724365, 3985.359647631645, 3958.1994038820267, 3930.737146615982, 3902.9481624364853, 3874.8254877328873, 3846.367839694023, 3817.579847216606, 3788.4297901391983, 3758.920683860779, 3729.034088075161, 3698.764029622078, 3668.1028084158897, 3637.0456870794296, 3605.597531557083, 3573.7474194169044, 3541.504622042179, 3508.839570879936, 3475.7910495996475, 3442.3424502015114, 3408.4904229044914, 3374.24497961998, 3339.59187489748, 3304.5736399888992, 3269.1848397254944, 3233.431045651436, 3197.3454149365425, 3160.921939611435, 3124.1803272366524, 3087.133918464184, 3049.803176701069, 3012.187026619911, 2974.3163761496544, 2936.1954375505447, 2897.8471872210503, 2859.3079118728638, 2820.5939969718456, 2781.718364804983, 2742.709654390812, 2703.5820414721966, 2664.383254110813, 2625.106814086437, 2585.80723631382, 2546.4776842594147, 2507.175718665123, 2467.90625821054, 2428.6913818120956, 2389.591281399131, 2350.5874355584383, 2311.718280404806, 2273.032006174326, 2234.5409931093454, 2196.2494055330753, 2158.198753505945, 2120.409622207284, 2082.891990378499, 2045.674878872931, 2008.7859189808369, 1972.2461713403463, 1936.0586200729012, 1900.2599980458617, 1864.872683852911]\n"
     ]
    }
   ],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = functional.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = functional.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    for context, target in trigrams:\n",
    "\n",
    "        # Converting each owrd in 1*10 sized tensor tensor\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "\n",
    "        # Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Run the forward pass, getting log probabilities over next words\n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "        # Compute your loss function. (Again, Torch wants the target word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "\n",
    "        # Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_venv",
   "language": "python",
   "name": "torch_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
